{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## A. Importing the required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os # to use operating system dependent functionality\nimport librosa # to extract speech features\nimport wave # read and write WAV files\nimport matplotlib.pyplot as plt # to generate the visualizations\n\n# MLP Classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# LSTM Classifier\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import rmsprop","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B. Reading the Dataset & Feature Extraction","metadata":{}},{"cell_type":"code","source":"def extract_mfcc(wav_file_name):\n    #This function extracts mfcc features and obtain the mean of each dimension\n    #Input : path_to_wav_file\n    #Output: mfcc_features'''\n    y, sr = librosa.load(wav_file_name)\n    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n    \n    return mfccs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### load radvess speech dataset #####\nradvess_speech_labels = [] # to save extracted label/file\nravdess_speech_data = [] # to save extracted features/file\nfor dirname, _, filenames in os.walk('/kaggle/input/ravdess-emotional-speech-audio/'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        radvess_speech_labels.append(int(filename[7:8]) - 1) # the index 7 and 8 of the file name represent the emotion label\n        wav_file_name = os.path.join(dirname, filename)\n        ravdess_speech_data.append(extract_mfcc(wav_file_name)) # extract MFCC features/file\n        \nprint(\"Finish Loading the Dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### convert data and label to array\nravdess_speech_data_array = np.asarray(ravdess_speech_data) # convert the input to an array\nravdess_speech_label_array = np.array(radvess_speech_labels)\nravdess_speech_label_array.shape # get tuple of array dimensions\n\n#### make categorical labels\nlabels_categorical = to_categorical(ravdess_speech_label_array) # converts a class vector (integers) to binary class matrix\nravdess_speech_data_array.shape\nlabels_categorical.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## C. Train and Test MLP Classifier","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test= train_test_split(np.array(ravdess_speech_data_array),labels_categorical, test_size=0.20, random_state=9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Multi Layer Perceptron Classifier\nmodel=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict for the test set\ny_pred=model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the accuracy of our model\naccuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n# Print the accuracy\nprint(\"Accuracy: {:.2f}%\".format(accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the training, validating, and testing sets\nnumber_of_samples = ravdess_speech_data_array.shape[0]\ntraining_samples = int(number_of_samples * 0.8)\nvalidation_samples = int(number_of_samples * 0.1)\ntest_samples = int(number_of_samples * 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the LSTM model\ndef create_model_LSTM():\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    \n    # Configures the model for training\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### train using LSTM model\nmodel_A = create_model_LSTM()\nhistory = model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples],-1), labels_categorical[:training_samples], validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1), labels_categorical[training_samples:training_samples+validation_samples]), epochs=100, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### loss plots using LSTM model\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### accuracy plots using LSTM model\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### evaluate using model A\nmodel_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], -1), labels_categorical[training_samples + validation_samples:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}